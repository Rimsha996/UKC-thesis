{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rimsha996/UKC-thesis/blob/main/K_fold_DensetNet121_2Cat_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ2dSKpJU9X5",
        "outputId": "27233514-2811-4943-a3e4-bd960456938e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "google_colab_flag = True # Make it False if do not intend to use Google Colab and want to train in local machine!!\n",
        "\n",
        "# For training in Google Colab\n",
        "if(google_colab_flag):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  !ls\n",
        "  import sys\n",
        "  root_path = '/content/drive/MyDrive/FlippedImages -removedBlackBorder/Data-resized(300,400)-2Categories - Copy/' # This is the path to where in google drive the code is stored!\n",
        "  sys.path.append(root_path)\n",
        "\n",
        "# For local training\n",
        "else:\n",
        "  root_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQTzpBUWVW1q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from PIL import Image\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "import warnings\n",
        "import os\n",
        "import shutil\n",
        "from PIL import ImageFile\n",
        "warnings.simplefilter('error', Image.DecompressionBombWarning)\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from PIL import Image\n",
        "Image.MAX_IMAGE_PIXELS = 100000000\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrmvoLCmVmgj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "datasetFolderName=root_path\n",
        "MODEL_FILENAME=root_path+\"model_DenseNet_2_cv.h5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(MODEL_FILENAME, save_best_only=True, save_weights_only=True, verbose=1 )\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "sourceFiles=[]\n",
        "classLabels=['mild','severe']\n",
        "X=[]\n",
        "Y=[]\n",
        "\n",
        "img_rows, img_cols =  400,300 # input image dimensions\n",
        "train_path=datasetFolderName+'/train/'\n",
        "validation_path=datasetFolderName+'/val/'\n",
        "test_path=datasetFolderName+'/test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyWadm9IVxbh"
      },
      "outputs": [],
      "source": [
        "def transferBetweenFolders(source, dest, splitRate):\n",
        "    global sourceFiles\n",
        "    sourceFiles=os.listdir(source)\n",
        "    if(len(sourceFiles)!=0):\n",
        "        transferFileNumbers=int(len(sourceFiles)*splitRate)\n",
        "        transferIndex=random.sample(range(0, len(sourceFiles)), transferFileNumbers)\n",
        "        for eachIndex in transferIndex:\n",
        "            shutil.move(source+str(sourceFiles[eachIndex]), dest+str(sourceFiles[eachIndex]))\n",
        "    else:\n",
        "        print(\"No file moved. Source empty!\")\n",
        "\n",
        "def transferAllClassBetweenFolders(source, dest, splitRate):\n",
        "    for label in classLabels:\n",
        "        transferBetweenFolders(datasetFolderName+'/'+source+'/'+label+'/',\n",
        "                               datasetFolderName+'/'+dest+'/'+label+'/',\n",
        "                               splitRate)\n",
        "\n",
        "def my_metrics(y_true, y_pred):\n",
        "    accuracy=accuracy_score(y_true, y_pred)\n",
        "    precision=precision_score(y_true, y_pred,average='weighted')\n",
        "    f1Score=f1_score(y_true, y_pred, average='weighted')\n",
        "    print(\"Accuracy  : {}\".format(accuracy))\n",
        "    print(\"Precision : {}\".format(precision))\n",
        "    print(\"f1Score : {}\".format(f1Score))\n",
        "    cm=confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    return accuracy, precision, f1Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_hY7yHHV3gS"
      },
      "outputs": [],
      "source": [
        "transferAllClassBetweenFolders('test', 'train', 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-f7yS6XV7Nv"
      },
      "outputs": [],
      "source": [
        "transferAllClassBetweenFolders('train', 'test', 0.30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGBVJqZmV7US"
      },
      "outputs": [],
      "source": [
        "def prepareNameWithLabels(folderName):\n",
        "    sourceFiles=os.listdir(datasetFolderName+'/train/'+folderName)\n",
        "    for val in sourceFiles:\n",
        "        X.append(val)\n",
        "        for i in range(len(classLabels)):\n",
        "          if(folderName==classLabels[i]):\n",
        "              Y.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxZ4fxfAV7Xa"
      },
      "outputs": [],
      "source": [
        "# Organize file names and class labels in X and Y variables\n",
        "for i in range(len(classLabels)):\n",
        "  prepareNameWithLabels(classLabels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HgrtkXOYJsc",
        "outputId": "ec166948-38aa-4de3-a49a-3745200adf19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['G1IMG0.jpg', 'G1IMG4.jpg', 'G1IMG1.jpg', 'G1IMG5.jpg', 'G1IMG6.jpg', 'G1IMG3.jpg', 'G1IMG9.jpg', 'G1IMG19.jpg', 'G1IMG26.jpg', 'G1IMG14.jpg', 'G1IMG18.jpg', 'G1IMG12.jpg', 'G1IMG13.jpg', 'G1IMG7.jpg', 'G1IMG27.jpg', 'G1IMG21.jpg', 'G1IMG16.jpg', 'G1IMG17.jpg', 'G1IMG8.jpg', 'G1IMG10.jpg', 'G1IMG15.jpg', 'G2IMG10.jpg', 'G2IMG12.jpg', 'G2IMG1.jpg', 'G2IMG9.jpg', 'G2IMG13.jpg', 'G2IMG19.jpg', 'G2IMG7.jpg', 'G2IMG11.jpg', 'G2IMG21.jpg', 'G2IMG6.jpg', 'G2IMG25.jpg', 'G2IMG8.jpg', 'G2IMG18.jpg', 'G2IMG26.jpg', 'G2IMG22.jpg', 'G2IMG5.jpg', 'G2IMG15.jpg', 'G2IMG20.jpg', 'G2IMG2.jpg', 'mild_original_G1IMG0.jpg_b5174d64-975b-4fba-911b-861bc5da2f49.jpg', 'mild_original_G2IMG27.jpg_27b605b1-e6f8-485e-ac00-12af88739956.jpg', 'mild_original_G2IMG6.jpg_52f45692-06e0-48ea-b483-8a029067c3af.jpg', 'mild_original_G1IMG14.jpg_bce39c07-a52a-40fb-b9ab-47d0440ba3e1.jpg', 'mild_original_G1IMG12.jpg_260b9b89-f51e-4fff-9193-3839378195b0.jpg', 'mild_original_G2IMG5.jpg_3e586b74-555a-4182-b699-877b5e37a5dc.jpg', 'mild_original_G1IMG27.jpg_fdaaf660-5ca3-4e9f-bca5-96c272d0c5cc.jpg', 'mild_original_G2IMG8.jpg_c77004e3-ef3d-4ef7-aa83-ae14f83059d9.jpg', 'mild_original_G2IMG12.jpg_c0370496-935e-411c-9064-b60a43bef667.jpg', 'mild_original_G2IMG2.jpg_f81ba2cd-f66f-4388-bd87-0643fa0bae39.jpg', 'mild_original_G1IMG13.jpg_474529b0-97bc-4c92-a99e-b407898ffd8d.jpg', 'mild_original_G2IMG24.jpg_6a6162fe-3763-4e2c-8260-4394d347b3cc.jpg', 'mild_original_G2IMG9.jpg_11c3d694-502d-49b8-a520-50c846f8278a.jpg', 'mild_original_G2IMG7.jpg_ce38117b-0e28-4c41-a641-d86a125cdeed.jpg', 'mild_original_G1IMG13.jpg_def02a77-3d13-46c3-b4d1-2a92b6091163.jpg', 'mild_original_G2IMG10.jpg_38f333d3-c151-4f31-886b-45e1055628d9.jpg', 'mild_original_G1IMG9.jpg_a1b69067-1255-49b2-bbcf-91e28d957611.jpg', 'mild_original_G1IMG11.jpg_3a57c258-2808-4d37-8b97-4c6ead911d83.jpg', 'mild_original_G2IMG14.jpg_c0a61884-a1af-4bb2-bcb4-d82111dc89c6.jpg', 'mild_original_G2IMG10.jpg_2ba7a7fa-153d-4423-8086-dd7220b63da0.jpg', 'mild_original_G2IMG26.jpg_63e8c3e5-83a0-404a-a29a-c0c46183fb62.jpg', 'mild_original_G1IMG17.jpg_0cc56eb9-d5c6-4af1-a690-ff372712725d.jpg', 'mild_original_G1IMG25.jpg_d561c20b-6b0b-45a4-a80b-80de9810e20c.jpg', 'mild_original_G1IMG1.jpg_1b9e8250-71b3-4bc0-8f02-5a393479e3fe.jpg', 'mild_original_G1IMG3.jpg_c0226d85-8045-4985-b878-6936e418c73f.jpg', 'mild_original_G1IMG17.jpg_254491a4-961f-4c8f-b262-c3aad81ce760.jpg', 'mild_original_G2IMG1.jpg_37f9cb24-1e53-4a3d-a093-eb46f58f53ea.jpg', 'mild_original_G1IMG23.jpg_2a91e78b-238a-4b68-9a17-28d02902ba3b.jpg', 'mild_original_G2IMG3.jpg_a61f82a7-6647-4b7f-8d37-67166df2c474.jpg', 'mild_original_G2IMG7.jpg_c344b282-be27-44c4-a559-e2ef3cc82199.jpg', 'mild_original_G2IMG8.jpg_043bf2d1-28e5-4847-983d-a70d0601f36c.jpg', 'mild_original_G2IMG0.jpg_d85248cd-c0a1-40f5-8871-57a869e998df.jpg', 'mild_original_G1IMG4.jpg_f335cc78-4e90-474f-a727-857a221cc84e.jpg', 'mild_original_G1IMG27.jpg_13043cb3-4c7e-42b3-974f-582a1b108c10.jpg', 'mild_original_G2IMG22.jpg_31233406-ca1e-4c04-b5f6-a40144e6c632.jpg', 'mild_original_G1IMG14.jpg_818e0163-ab90-4318-956e-85275449642f.jpg', 'mild_original_G2IMG17.jpg_ee227997-b47f-4d0e-94a9-d39508393a60.jpg', 'mild_original_G1IMG10.jpg_f3d31b3c-8e7d-477f-9ac9-4c4eae427bd3.jpg', 'mild_original_G2IMG8.jpg_a6d7fb4a-8aeb-412b-90e1-6b9b4fc0d3ba.jpg', 'mild_original_G2IMG24.jpg_70e8e791-1c02-4129-961c-f72aaf13448d.jpg', 'mild_original_G1IMG27.jpg_b6cf665b-550a-4b8d-9bdd-22b00226db86.jpg', 'mild_original_G2IMG25.jpg_52e55a1b-005e-47d3-9d8f-2de537a05e0f.jpg', 'mild_original_G2IMG4.jpg_7bc5fd17-cd3d-4d9d-861a-67b391113986.jpg', 'mild_original_G1IMG12.jpg_7b80ef49-7a97-47dd-bfe5-4ae7b0264542.jpg', 'mild_original_G2IMG7.jpg_a25bdf8f-9f8f-48d4-b953-cf6619fd9e76.jpg', 'mild_original_G1IMG21.jpg_387b6ec2-c209-45c1-9b7d-0681c29c65ba.jpg', 'mild_original_G2IMG1.jpg_ab0842ef-1311-42a6-8396-15836b413b6e.jpg', 'mild_original_G1IMG25.jpg_f556845a-8f9f-40c8-860b-c4312758eaf5.jpg', 'mild_original_G2IMG5.jpg_9617a8d2-e1ba-4a93-b0ab-0516da239fc4.jpg', 'mild_original_G2IMG2.jpg_d88b8a11-e683-4ed0-88b2-848265f6dc5a.jpg', 'mild_original_G1IMG15.jpg_3235afef-3151-4ec1-84b9-36c14b3c6ceb.jpg', 'mild_original_G1IMG14.jpg_c4d58baf-c03b-4d4e-8ee1-da43538bde0b.jpg', 'mild_original_G1IMG8.jpg_5100033c-9f61-4376-af1c-62f46c8dae98.jpg', 'mild_original_G2IMG2.jpg_5b7bdd18-c712-4d0a-8f09-2c3907ddd110.jpg', 'mild_original_G1IMG15.jpg_6d81ee75-ed0f-4dab-a8d5-9efa15fe0332.jpg', 'mild_original_G2IMG4.jpg_5dc5f2fb-8628-43c3-8dd3-db76b239375a.jpg', 'mild_original_G1IMG10.jpg_08b782df-e5ad-4b33-b6ce-770ac997eb3a.jpg', 'mild_original_G1IMG3.jpg_5d832fee-8c56-4660-94f8-a738933c1dc9.jpg', 'mild_original_G2IMG13.jpg_47d68ac1-574c-4cf2-b29c-a805e78ccacc.jpg', 'mild_original_G2IMG6.jpg_717e3d32-de90-447c-8f21-370a754bbf1c.jpg', 'mild_original_G2IMG10.jpg_869cba97-8a3c-42da-b1d5-9314395cfdf6.jpg', 'mild_original_G2IMG10.jpg_627d77c4-f26b-4127-95fd-fb55d22ef73e.jpg', 'mild_original_G1IMG8.jpg_f84a52b7-8b33-460f-9af3-49ea2b3248ac.jpg', 'mild_original_G2IMG18.jpg_cca8305b-7d5c-465a-9ad1-23428d5be0c3.jpg', 'mild_original_G2IMG27.jpg_88fe7e3a-9745-4547-8ca0-be058deb5990.jpg', 'G3IMG4.jpg', 'G3IMG2.jpg', 'G3IMG7.jpg', 'G3IMG1.jpg', 'G3IMG8.jpg', 'G3IMG16.jpg', 'G3IMG21.jpg', 'G3IMG23.jpg', 'G3IMG13.jpg', 'G3IMG3.jpg', 'G3IMG15.jpg', 'G3IMG26.jpg', 'G3IMG10.jpg', 'G3IMG25.jpg', 'G3IMG24.jpg', 'G3IMG22.jpg', 'G3IMG14.jpg', 'G3IMG6.jpg', 'G3IMG11.jpg', 'G3IMG18.jpg', 'G3IMG19.jpg', 'G3IMG12.jpg', 'G3IMG27.jpg', 'G4IMG1.jpg', 'G4IMG0.jpg', 'G4IMG5.jpg', 'G4IMG12.jpg', 'G4IMG4.jpg', 'G4IMG3.jpg', 'G4IMG7.jpg', 'G4IMG13.jpg', 'G4IMG9.jpg', 'G4IMG14.jpg', 'G4IMG15.jpg', 'G4IMG16.jpg', 'G4IMG17.jpg', 'G4IMG19.jpg', 'severe_original_G3IMG11.jpg_0b3aa5ab-dee5-4308-a5f3-82b31cda269c.jpg', 'severe_original_G3IMG0.jpg_0b4235c0-07b6-4fd1-b37d-f311e8b6108e.jpg', 'severe_original_G3IMG6.jpg_1e45cccf-3cf8-42ab-a539-a835bc300fa0.jpg', 'severe_original_G4IMG6.jpg_53f2abe5-834a-49da-b6b1-1002ae3051a2.jpg', 'severe_original_G3IMG5.jpg_14f15390-5063-4d44-98cd-ea00ed408f37.jpg', 'severe_original_G3IMG6.jpg_85209044-9408-4195-ab0f-4c395603a244.jpg', 'severe_original_G4IMG5.jpg_c9c629e2-6e7b-4261-890c-8530ae00783e.jpg', 'severe_original_G3IMG11.jpg_b916199b-9f84-409f-9941-074b226197e9.jpg', 'severe_original_G4IMG9.jpg_68b3e529-791d-48b6-b5e0-0d4aef728052.jpg', 'severe_original_G3IMG23.jpg_7bee8c00-f598-4c96-b2c0-24a338d06da9.jpg', 'severe_original_G3IMG9.jpg_5700abbe-6794-4702-811c-6ba7b8f85ee9.jpg', 'severe_original_G4IMG9.jpg_96a60272-9ea6-4803-bf76-e75cba33347e.jpg', 'severe_original_G3IMG6.jpg_e77520c2-abb4-4d4d-a71b-46afd94b51b5.jpg', 'severe_original_G3IMG4.jpg_b9d119a8-e2bd-4545-be2e-6ea07b823801.jpg', 'severe_original_G3IMG4.jpg_1d2e3426-95b1-4086-add2-d65f22b5fd6c.jpg', 'severe_original_G4IMG1.jpg_100d155b-76bd-40c7-a47f-f00b26447977.jpg', 'severe_original_G3IMG1.jpg_20f18039-4e3f-499c-9cfb-26d72535b189.jpg', 'severe_original_G3IMG20.jpg_bc9f3f7d-b334-4ccb-a36f-c2f82726850b.jpg', 'severe_original_G4IMG11.jpg_f64dd34d-1c03-4f34-825b-d3d4c41768d1.jpg', 'severe_original_G4IMG19.jpg_85c40aa3-616b-4957-b1f7-5a903556bac3.jpg', 'severe_original_G4IMG15.jpg_e412c5e3-417d-498b-9c85-ec5a3d5fafd7.jpg', 'severe_original_G3IMG13.jpg_2a0eee42-be77-4b7f-8656-749df5807fe8.jpg', 'severe_original_G4IMG9.jpg_83f7d83b-7b22-424c-ae58-80ed5cbb4ac6.jpg', 'severe_original_G3IMG5.jpg_f000fc41-0662-4dd1-a4f5-18094774ae8a.jpg', 'severe_original_G3IMG3.jpg_d16f94a9-5555-407f-8f3e-14f013e50241.jpg', 'severe_original_G3IMG21.jpg_c7c278ec-eb2f-4988-a42b-a1070193f860.jpg', 'severe_original_G3IMG24.jpg_254d12be-48b7-46ab-8e8a-7593b81220bd.jpg', 'severe_original_G3IMG19.jpg_00a7cd1b-12d0-4b47-9e9e-93aaa2b3fefe.jpg', 'severe_original_G4IMG19.jpg_cc6ecf04-f220-491b-b452-cd702c077bbd.jpg', 'severe_original_G3IMG0.jpg_016dbfe5-7b8f-470b-a8c8-bff2ffb26c7e.jpg', 'severe_original_G3IMG24.jpg_11c50bd8-d5a9-49b6-a610-29bb84a6ccb8.jpg', 'severe_original_G4IMG6.jpg_0c6ced42-3aed-424a-b05f-42265a37fc64.jpg', 'severe_original_G3IMG18.jpg_342f6076-9d50-4583-b4f3-2aa31eee8716.jpg', 'severe_original_G4IMG16.jpg_d17b1340-56ce-4b3c-8581-7060352c24a4.jpg', 'severe_original_G3IMG14.jpg_df4d152b-8e65-4012-af64-839203eecfab.jpg', 'severe_original_G3IMG10.jpg_d900a52d-6565-4da5-8148-61587e5cc642.jpg', 'severe_original_G4IMG7.jpg_9930d6ab-27ab-48ee-b8ce-1cd3e7a7ae22.jpg', 'severe_original_G3IMG18.jpg_ccd66a91-bccf-4d61-8265-5e932fc356b7.jpg', 'severe_original_G4IMG17.jpg_e3a36849-5f5b-4e51-8a12-a8b27b59c187.jpg', 'severe_original_G3IMG20.jpg_a34df4a8-73d5-4fdb-8a55-03738e0b73bc.jpg', 'severe_original_G4IMG12.jpg_c3c50658-f59c-4ade-a910-850cfacb410b.jpg', 'severe_original_G3IMG12.jpg_4b42863e-933d-4d73-89c1-5e5832616811.jpg', 'severe_original_G3IMG20.jpg_31613dd6-3b28-4ed6-836a-5d011d16f6d8.jpg', 'severe_original_G4IMG11.jpg_f8d60f27-5e3f-488d-8d68-6e101631219a.jpg', 'severe_original_G3IMG19.jpg_c161a7eb-641c-4f19-9615-beb7ff27cdc7.jpg', 'severe_original_G4IMG19.jpg_a5e58c10-012e-4663-853f-3cba7a475b06.jpg', 'severe_original_G4IMG16.jpg_443a8367-8851-4d92-afd9-5deaaa315c9c.jpg', 'severe_original_G3IMG25.jpg_6e3e2afd-351e-4b90-9ac6-ddbc2691ff68.jpg', 'severe_original_G3IMG17.jpg_a99ee11b-904a-4703-ba8f-e75190d5e9ed.jpg', 'severe_original_G4IMG7.jpg_5420e557-32c3-482a-a34d-5f454b74aebb.jpg', 'severe_original_G4IMG2.jpg_7f0d1704-32d4-4faa-812a-f833ee47ae2c.jpg', 'severe_original_G3IMG10.jpg_62279a97-047c-4f78-af0c-5be6fd699b48.jpg', 'severe_original_G3IMG26.jpg_851fcfee-6cf3-4a86-917e-3d1d7a423c88.jpg', 'severe_original_G3IMG16.jpg_3a6dc2de-8bc4-4abf-9296-983f08949b84.jpg', 'severe_original_G4IMG9.jpg_f3b6f77b-266e-4d0b-8866-3689ebe22ff9.jpg', 'severe_original_G4IMG5.jpg_291c186a-b236-40de-bb1c-cf1942b5f005.jpg', 'severe_original_G3IMG20.jpg_cd2cc7f7-d67b-4823-b636-e579b159e93e.jpg', 'severe_original_G3IMG1.jpg_dac2a0c4-cafa-4624-a689-71009748623c.jpg', 'severe_original_G4IMG3.jpg_91bc16df-780a-46b6-985e-f48dbfa967f3.jpg']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "print(X)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU4J_vr3V7Zz"
      },
      "outputs": [],
      "source": [
        "X=np.asarray(X)\n",
        "Y=np.asarray(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaRWmmh6Wgtu",
        "outputId": "13ce735e-2ab8-4b40-b26d-2d7ca1668352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras import models\n",
        "\n",
        "conv_base=DenseNet121(weights='imagenet',include_top=False,input_shape=(300,400,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke7XMraYV7cz"
      },
      "outputs": [],
      "source": [
        "# Note that, this model structure is a very basic one. To achieve better performance, you should change the model structure and hyperparameters according to your needs and data.\n",
        "batch_size = 16\n",
        "epoch=50\n",
        "def getModel():\n",
        "    model = models.Sequential()\n",
        "    model.add(conv_base)\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(Dense(len(classLabels), activation='softmax'))\n",
        "    # model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adagrad(),metrics=['accuracy'])\n",
        "    model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model=getModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQrpdvKzXEza",
        "outputId": "df2ddc8d-1b99-4dc8-cf0a-c9729e63d224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for fold 1\n",
            "Found 201 images belonging to 2 classes.\n",
            "Found 67 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "8/8 [==============================] - 232s 27s/step - loss: 2.5711 - accuracy: 0.5124\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 212s 26s/step - loss: 0.6575 - accuracy: 0.6116\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 215s 27s/step - loss: 0.5574 - accuracy: 0.7190\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 223s 28s/step - loss: 0.5697 - accuracy: 0.7031\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 210s 26s/step - loss: 0.4541 - accuracy: 0.7851\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 225s 28s/step - loss: 0.5231 - accuracy: 0.7500\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 225s 28s/step - loss: 0.5159 - accuracy: 0.7266\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 237s 30s/step - loss: 0.4595 - accuracy: 0.7578\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 240s 30s/step - loss: 0.4379 - accuracy: 0.8047\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 240s 30s/step - loss: 0.4209 - accuracy: 0.8047\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 225s 28s/step - loss: 0.4181 - accuracy: 0.7851\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 240s 30s/step - loss: 0.4439 - accuracy: 0.7812\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 215s 26s/step - loss: 0.3636 - accuracy: 0.8182\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 212s 26s/step - loss: 0.3661 - accuracy: 0.8430\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 236s 29s/step - loss: 0.3585 - accuracy: 0.8359\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 234s 29s/step - loss: 0.2956 - accuracy: 0.8984\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 222s 27s/step - loss: 0.2830 - accuracy: 0.8843\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 223s 28s/step - loss: 0.2447 - accuracy: 0.9008\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 220s 27s/step - loss: 0.2360 - accuracy: 0.9256\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 235s 29s/step - loss: 0.2353 - accuracy: 0.8906\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 222s 27s/step - loss: 0.2286 - accuracy: 0.9174\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 233s 29s/step - loss: 0.2184 - accuracy: 0.9062\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 222s 27s/step - loss: 0.4517 - accuracy: 0.7851\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 231s 29s/step - loss: 0.1756 - accuracy: 0.9375\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 219s 27s/step - loss: 0.1894 - accuracy: 0.9091\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 223s 27s/step - loss: 0.2388 - accuracy: 0.9008\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 232s 29s/step - loss: 0.2133 - accuracy: 0.8984\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 230s 29s/step - loss: 0.1412 - accuracy: 0.9609\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 219s 27s/step - loss: 0.1850 - accuracy: 0.9256\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 229s 29s/step - loss: 0.1824 - accuracy: 0.9297\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 217s 27s/step - loss: 0.1328 - accuracy: 0.9587\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 217s 27s/step - loss: 0.1679 - accuracy: 0.9504\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 218s 29s/step - loss: 0.2319 - accuracy: 0.9008\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 216s 27s/step - loss: 0.1888 - accuracy: 0.9339\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 229s 29s/step - loss: 0.1402 - accuracy: 0.9609\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 216s 27s/step - loss: 0.1555 - accuracy: 0.9504\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 217s 27s/step - loss: 0.1362 - accuracy: 0.9669\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 217s 27s/step - loss: 0.1502 - accuracy: 0.9504\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 217s 27s/step - loss: 0.1329 - accuracy: 0.9421\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 216s 27s/step - loss: 0.1165 - accuracy: 0.9669\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 229s 29s/step - loss: 0.0985 - accuracy: 0.9766\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 217s 27s/step - loss: 0.0564 - accuracy: 0.9917\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 216s 29s/step - loss: 0.0754 - accuracy: 0.9835\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 228s 28s/step - loss: 0.0948 - accuracy: 0.9766\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 216s 27s/step - loss: 0.1109 - accuracy: 0.9504\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 230s 29s/step - loss: 0.0503 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 217s 27s/step - loss: 0.1244 - accuracy: 0.9421\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 229s 29s/step - loss: 0.1056 - accuracy: 0.9453\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 216s 27s/step - loss: 0.0660 - accuracy: 0.9835\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 203s 25s/step - loss: 0.0718 - accuracy: 0.9917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 21s 4s/step\n",
            "***Performance on Validation data***\n",
            "Accuracy  : 0.9402985074626866\n",
            "Precision : 0.9417238133656044\n",
            "f1Score : 0.9401647675600493\n",
            "[[34  1]\n",
            " [ 3 29]]\n",
            "Results for fold 2\n",
            "Found 201 images belonging to 2 classes.\n",
            "Found 67 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "8/8 [==============================] - 202s 25s/step - loss: 0.1076 - accuracy: 0.9587\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 201s 25s/step - loss: 0.1030 - accuracy: 0.9587\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 215s 27s/step - loss: 0.1282 - accuracy: 0.9531\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 212s 27s/step - loss: 0.1737 - accuracy: 0.9141\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 199s 25s/step - loss: 0.0860 - accuracy: 0.9752\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 202s 25s/step - loss: 0.0926 - accuracy: 0.9752\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 201s 27s/step - loss: 0.0392 - accuracy: 0.9917\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 202s 25s/step - loss: 0.1024 - accuracy: 0.9587\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 202s 25s/step - loss: 0.0894 - accuracy: 0.9587\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 204s 25s/step - loss: 0.0821 - accuracy: 0.9752\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 200s 25s/step - loss: 0.1010 - accuracy: 0.9688\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0739 - accuracy: 0.9766\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0594 - accuracy: 0.9752\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.1112 - accuracy: 0.9669\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0384 - accuracy: 0.9844\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0213 - accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0545 - accuracy: 0.9752\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0723 - accuracy: 0.9922\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0741 - accuracy: 0.9688\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0647 - accuracy: 0.9688\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0332 - accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0946 - accuracy: 0.9587\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 185s 23s/step - loss: 0.0360 - accuracy: 0.9922\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0423 - accuracy: 0.9917\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0624 - accuracy: 0.9752\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0600 - accuracy: 0.9917\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0244 - accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0466 - accuracy: 0.9922\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0820 - accuracy: 0.9688\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0335 - accuracy: 0.9922\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0376 - accuracy: 0.9835\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0398 - accuracy: 0.9835\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0619 - accuracy: 0.9669\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0299 - accuracy: 0.9922\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0121 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0188 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0174 - accuracy: 0.9922\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0282 - accuracy: 0.9917\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0286 - accuracy: 0.9835\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0235 - accuracy: 0.9917\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 176s 23s/step - loss: 0.0346 - accuracy: 0.9917\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 185s 23s/step - loss: 0.0638 - accuracy: 0.9844\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0563 - accuracy: 0.9752\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 185s 23s/step - loss: 0.0382 - accuracy: 0.9844\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 185s 23s/step - loss: 0.0553 - accuracy: 0.9844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 18s 3s/step\n",
            "***Performance on Validation data***\n",
            "Accuracy  : 1.0\n",
            "Precision : 1.0\n",
            "f1Score : 1.0\n",
            "[[35  0]\n",
            " [ 0 32]]\n",
            "Results for fold 3\n",
            "Found 201 images belonging to 2 classes.\n",
            "Found 67 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0312 - accuracy: 1.0000\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 185s 23s/step - loss: 0.0222 - accuracy: 1.0000\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 185s 23s/step - loss: 0.0189 - accuracy: 0.9922\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0384 - accuracy: 0.9917\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 185s 23s/step - loss: 0.0162 - accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0269 - accuracy: 0.9917\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0240 - accuracy: 0.9917\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0231 - accuracy: 0.9844\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0376 - accuracy: 0.9844\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0208 - accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0321 - accuracy: 0.9917\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 175s 23s/step - loss: 0.0534 - accuracy: 0.9752\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 175s 23s/step - loss: 0.0173 - accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0234 - accuracy: 0.9917\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 176s 22s/step - loss: 0.0162 - accuracy: 0.9917\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0417 - accuracy: 0.9844\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 186s 23s/step - loss: 0.0099 - accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0264 - accuracy: 0.9835\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 175s 22s/step - loss: 0.0307 - accuracy: 0.9917\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 176s 23s/step - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 178s 22s/step - loss: 0.0140 - accuracy: 0.9917\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 178s 23s/step - loss: 0.0163 - accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 178s 22s/step - loss: 0.0303 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 188s 23s/step - loss: 0.0825 - accuracy: 0.9609\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 188s 23s/step - loss: 0.0141 - accuracy: 0.9922\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 188s 23s/step - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 178s 23s/step - loss: 0.0190 - accuracy: 0.9917\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 178s 22s/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 180s 22s/step - loss: 0.0428 - accuracy: 0.9752\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 208s 26s/step - loss: 0.0174 - accuracy: 0.9922\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 204s 25s/step - loss: 0.0406 - accuracy: 0.9835\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 206s 27s/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 216s 27s/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 216s 27s/step - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 217s 27s/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 217s 27s/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 219s 27s/step - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 205s 25s/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 214s 27s/step - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 196s 24s/step - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 202s 25s/step - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 211s 26s/step - loss: 0.0156 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 201s 25s/step - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 207s 26s/step - loss: 0.0118 - accuracy: 0.9922\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 198s 26s/step - loss: 0.0150 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 195s 24s/step - loss: 0.0147 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 197s 25s/step - loss: 0.0040 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 19s 4s/step\n",
            "***Performance on Validation data***\n",
            "Accuracy  : 1.0\n",
            "Precision : 1.0\n",
            "f1Score : 1.0\n",
            "[[35  0]\n",
            " [ 0 32]]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# ===============Stratified K-Fold======================\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
        "skf.get_n_splits(X, Y)\n",
        "foldNum=0\n",
        "\n",
        "for train_index, val_index in skf.split(X, Y):\n",
        "    #First cut all images from validation to train (if any exists)\n",
        "    transferAllClassBetweenFolders('val', 'train', 1.0)\n",
        "    foldNum+=1\n",
        "    print(\"Results for fold\",foldNum)\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
        "    # Move validation images of this fold from train folder to the validation folder\n",
        "    for eachIndex in range(len(X_val)):\n",
        "        classLabel=''\n",
        "        for i in range(len(classLabels)):\n",
        "          if(Y_val[eachIndex]==i):\n",
        "              classLabel=classLabels[i]\n",
        "        #Then, copy the validation images to the validation folder\n",
        "        shutil.move(datasetFolderName+'/train/'+classLabel+'/'+X_val[eachIndex],\n",
        "                    datasetFolderName+'/val/'+classLabel+'/'+X_val[eachIndex])\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "                      rescale=1./255,\n",
        "                      rotation_range=40,\n",
        "                      width_shift_range=0.2,\n",
        "                      height_shift_range=0.2,\n",
        "                      shear_range=0.2,\n",
        "                      zoom_range=0.2)\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    #Start ImageClassification Model\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset='training')\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_path,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,  # only data, no labels\n",
        "        shuffle=False)\n",
        "\n",
        "    # fit model\n",
        "    history=model.fit_generator(train_generator,\n",
        "                        steps_per_epoch=math.ceil(len(X_train)//batch_size),\n",
        "                        # callbacks = callbacks_list,\n",
        "                        epochs=epoch)\n",
        "    # history = model.fit_generator(\n",
        "    # train_generator ,\n",
        "    # steps_per_epoch=math.ceil(X_train//batch_size),\n",
        "    # epochs=epoch,\n",
        "    # callbacks=callbacks_list,\n",
        "    # validation_data=validation_generator,\n",
        "    # validation_steps=math.ceil(number_of_test_samples//batch_size)\n",
        "    # )\n",
        "\n",
        "    predictions = model.predict_generator(validation_generator, verbose=1)\n",
        "    yPredictions = np.argmax(predictions, axis=1)\n",
        "    true_classes = validation_generator.classes\n",
        "    # evaluate validation performance\n",
        "    print(\"***Performance on Validation data***\")\n",
        "    valAcc, valPrec, valFScore = my_metrics(true_classes, yPredictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28SWvWQ8XbW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39f8aee-5caa-422b-c993-1864e73e618a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============TEST RESULTS============\n",
            "Found 56 images belonging to 2 classes.\n",
            "4/4 [==============================] - 21s 4s/step\n",
            "Accuracy  : 1.0\n",
            "Precision : 1.0\n",
            "f1Score : 1.0\n",
            "[[28  0]\n",
            " [ 0 28]]\n"
          ]
        }
      ],
      "source": [
        "print(\"==============TEST RESULTS============\")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_path,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "predictions = model.predict(test_generator, verbose=1)\n",
        "yPredictions = np.argmax(predictions, axis=1)\n",
        "true_classes = test_generator.classes\n",
        "\n",
        "testAcc,testPrec, testFScore = my_metrics(true_classes, yPredictions)\n",
        "model.save(MODEL_FILENAME)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}